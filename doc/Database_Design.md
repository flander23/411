# Database Design

## Database
    CREATE TABLE `Customer` (
        `customer_id` INT PRIMARY KEY NOT NULL,
        `username` VARCHAR(50) NOT NULL,
        `password` VARCHAR(50) NOT NULL,
        `email` VARCHAR(100) NOT NULL,
        `first_name` VARCHAR(50) NOT NULL,
        `last_name` VARCHAR(50) NOT NULL,
        `phone_number` VARCHAR(50) NOT NULL
    );

    CREATE TABLE `Shop` (
        `shop_id` INT PRIMARY KEY NOT NULL,
        `shop_name` VARCHAR(50) NOT NULL,
        `shop_address` VARCHAR(255) NOT NULL
    );

    CREATE TABLE `Pet` (
        `pet_id` INT PRIMARY KEY NOT NULL,
        `shop_id` INT NOT NULL, 
        `customer_id` INT NOT NULL, 
        `color` VARCHAR(50) NOT NULL, 
        `type` VARCHAR(50) NOT NULL,
        `age` INT NOT NULL,
        `breed` VARCHAR(100) NOT NULL,
        FOREIGN KEY (`shop_id`) REFERENCES `Shop` (`shop_id`),
        FOREIGN KEY (`customer_id`) REFERENCES `Customer` (`customer_id`)
    );

    CREATE TABLE `Item` (
        `item_id` INT PRIMARY KEY NOT NULL,
        `shop_id` INT NOT NULL,
        `item_name` VARCHAR(50) NOT NULL,
        `type` VARCHAR(50) NOT NULL,
        `price` REAL NOT NULL,
        FOREIGN KEY (`shop_id`) REFERENCES `Shop` (`shop_id`)
    );

    CREATE TABLE `P_Order` (
        `order_id` INT PRIMARY KEY NOT NULL,
        `customer_id` INT NOT NULL,
        `total_price` REAL NOT NULL,
        `transaction_time` DATETIME NOT NULL,
        FOREIGN KEY (`customer_id`) REFERENCES `Customer` (`customer_id`)
    );

    CREATE TABLE `Contains` (
        `item_id` INT NOT NULL,
        `order_id` INT NOT NULL,
        `quantity` INT NOT NULL,
        PRIMARY KEY(`item_id`, `order_id`),
        FOREIGN KEY (`item_id`) REFERENCES `Item` (`item_id`),
        FOREIGN KEY (`order_id`) REFERENCES `P_Order` (`order_id`)
    );

##### Screenshot of successful connection and count for each table
![](https://github-dev.cs.illinois.edu/sp22-cs411/sp22-cs411-team059-TeamAhYesYes/blob/main/doc/img/database_screenshot.png?raw=true)

## Data Sources
- Data in table `Pet` is from [Shelter Animal Outcomes (Kaggle)](https://www.kaggle.com/c/shelter-animal-outcomes/data): train.csv
- Data in table `Item` is from [Pet Store Records 2020 (Kaggle)](https://www.kaggle.com/ippudkiippude/pet-store-records-2020)
- Data in table `Customer` is random generated from [generatedata.com](https://generatedata.com/)
- Data in table `Shop`, `Order`, `Contains` are generated by us based on local information and data from above tables.

## Advanced SQL Queries
### First advance query
	(SELECT 
	    i.item_id, s.shop_name, i.price
	FROM
	    item i
	        JOIN
	    shop s USING (shop_id)
	WHERE
	    i.type = 'Toys'
	        AND i.price > (SELECT 
	            AVG(price)
	        FROM
	            item
	        WHERE
	            shop_id = 1
	        GROUP BY shop_id)
	ORDER BY i.price) 
	UNION 
	(SELECT 
	    i.item_id, s.shop_name, i.price
	FROM
	    item i
	        JOIN
	    shop s USING (shop_id)
	WHERE
	    i.type = 'Food'
	        AND i.price > (SELECT 
	            AVG(price)
	        FROM
	            item
	        WHERE
	            shop_id = 2
	        GROUP BY shop_id)
	ORDER BY i.price) 
	LIMIT 15;

For the first advance query, we searched for all item_id, shop_name, and price where the price is greater than the average price, 
the item type is a toy, and it was purchased in the shop 1. And all item_id, shop_name, and price where the price is greater than the average price, 
the item type is a food, and it was purchased in the shop 2. Besides, we limit the sorting conditions. 

It satisfies all of four requirement:

- Join of multiple relations
- Set operations
- Aggregation via GROUP BY
- Subqueries

##### screenshots with the top 15 rows of the query results
![fd9b66d18d829eef148121f2133cb10](https://media.github-dev.cs.illinois.edu/user/14660/files/5c531df2-de12-4e98-9210-f0942e7544d1)

### Second advance query
	SELECT distinct p.customer_id, username, p.order_id
	FROM
	    p_order p
	        JOIN
	    customer USING (customer_id)
	        JOIN
	    contains c ON p.order_id = c.order_id
	        JOIN
	    item USING (item_id)
	WHERE
	    (transaction_time BETWEEN '2022-02-01' AND '2022-03-31')
	        AND item_id IN (SELECT 
	            item_id
	        FROM
	            item
	        WHERE
	            price > 100)
	order by customer_id
	LIMIT 15;

For the second advance query, we select customerId username and the orderId without repeating which transaction time is between this year 02/01 and 03/31 and also the item price is bigger then 100. We also limit the sorting condition with customerId.

It satisfies three of four requirement:

- Join of multiple relations
- Aggregation via GROUP BY
- Subqueries

##### screenshots with the top 15 rows of the query results
![9b1b34f3b5d9a432b3410c60bad1b6e](https://media.github-dev.cs.illinois.edu/user/14660/files/bbd2be67-9499-4813-8125-c3fd8aec9efb)


## Indexing Analysis
Throughout our analysis, we make our decisions primarily using the actual time statistics returned
by EXPLAIN ANALYZE. In particular, we use the total time to fetch all the tuples instead of the time
to fetch only the first one. We choose to look at this number since we believe this is more reflective of
the benefit of scalability that certain indexing schema brings.

### 1. Designing Indexes for the first query
The first query that we want to optimize is:
##### Query 1
![Query 1](https://github-dev.cs.illinois.edu/sp22-cs411/sp22-cs411-team059-TeamAhYesYes/blob/main/doc/img/indexes_analysis_1.jpg?raw=true)

We notice that we used item(item id, shop id, price, type) at various places of this query. Therefore,
a natural approach is to consider all different subsets of these 4 attributes. In fact, our definition of
the item table dictates that item id be the primary key and shop id be a foreign key, so we canâ€™t really
drop indexes on these two columns. This limits our possible subsets to:
1. item(item id, shop id)
2. item(item id, shop id, price)
3. item(item id, shop id, type)
4. item(item id, shop id, price, type).

Update: we also give an explanation why each of the above can potentially be a good selection:
1. This could be a good configuration because it involves all the attributes used in the join operations. 
It is reasonable to expect that the join operation is one of the more expensive operation among all operations, and therefore it makes
sense to try to optimize this more expensive part first. Additionally, shop_id is also heavily used in the where clause, and so the 
Vanilla configuration optimizes the where operation as well.
2. This could be a good configuration because in addition to optimizing the join time as in Vanilla, we now also optimize the where clause,
we now optimize the where clause even more. The where clause included a condition that the item we select is more expensive than the average, 
so we thought giving the additional index on price would allow faster execution because a full table scan on price can now be replaced with a
price index look-up. However, this should not help the aggregation avg(price), which must scan through the entire table anyway.
3. This could be a good configuration because in addition to optimizing the join time as in Vanilla, we now also optimize the where clause,
we now optimize the where clause even more. The where clause limited the type of product that is being selected, so we thought giving the additional 
index on type would allow faster execution because a full table scan on price can now be replaced with a price index look-up. Empirically, this does
turn out to be the best.
4. We thought this could be a good configuration because everything that ever appeared in the query is now indexed. In principle, this means the 
engine has the potential to turn every full table scan into a simple index look up, and thus avoiding scanning the table altogether. 

We explored all 4 and summarize our results as follows:

#### 1) (item(item id, shop id)):
##### Query 1 with indexes item(item id, shop id)
![Query 1 with indexes item(item id, shop id)](https://github-dev.cs.illinois.edu/sp22-cs411/sp22-cs411-team059-TeamAhYesYes/blob/main/doc/img/indexes_analysis_2.jpg?raw=true)

This would be the baseline model we use, as the indexes are automatically generated by the
indexes optimizer and none of the indexes can be removed.

#### 2) (item(item id, shop id, price)):
##### Query 1 with indexes item(item id, shop id, price)
![Query 1 with indexes item(item id, shop id, price)](https://github-dev.cs.illinois.edu/sp22-cs411/sp22-cs411-team059-TeamAhYesYes/blob/main/doc/img/indexes_analysis_3.jpg?raw=true)

When compared with the baseline, we see that while the innermost operation changed from a
Filter to a Table scan on s, which brought about a benefit of saving of around 2.6 seconds on
the running time, the other operations actually were worse-off. Indeed, the second operation
union now costs about 8 seconds more, and likewise for the nested loop inner join. Therefore, we
consider this schema not viable.

#### 3) (item(item id, shop id, type)):
##### Query 1 with indexes item(item id, shop id, type)
![Query 1 with indexes item(item id, shop id, type)](https://github-dev.cs.illinois.edu/sp22-cs411/sp22-cs411-team059-TeamAhYesYes/blob/main/doc/img/indexes_analysis_4.jpg?raw=true)

When compared with the baseline, we see that all 3 operations except the outermost one have
improved. Union improved by roughly 4 seconds, nested loop inner join by about 1.4 seconds,
and filter by around 1.4 seconds as well. Therefore, we consider this schema to be a viable option.

#### 4) (item(item id, shop id, price, type)):
##### Query 1 with indexes item(item id, shop id, price, type)
![Query 1 with indexes item(item id, shop id, price, type)](https://github-dev.cs.illinois.edu/sp22-cs411/sp22-cs411-team059-TeamAhYesYes/blob/main/doc/img/indexes_analysis_5.jpg?raw=true)

When compared with the baseline, we see that all 3 operations except the outermost one have
improved. Therefore, we rather compare this schema with the previous schema we considered.
Surprisingly, we notice that all 3 operations are faster than their counterparts from the previous
schema. This difference is especially pronounced for the last operation where the filter was replaced
by a scan. We consider this schema to be the best one if we donâ€™t consider the space complexity
and the associated update costs associated with this schema.

After considering all 4 schemas, we eventually decided that we will select schema 3 where we created
indexes on item id, shop id, type. Although schema 4 does better than schema 3, it was not by a big margin.
However, if we start to think about the space complexity and the cost of maintaining an extra index,
schema 3 outshines schema 4.

### 2. Designing Indexes for the second query
The second query that we want to optimize is:
##### Query 2
![Query 2](https://github-dev.cs.illinois.edu/sp22-cs411/sp22-cs411-team059-TeamAhYesYes/blob/main/doc/img/indexes_analysis_6.jpg?raw=true)

In addition to the primary keys and foreign keys whose indexes was automatically created and canâ€™t
be dropped, we notice that we used p order(transaction time) and customer(username) at various
places of this query. Therefore, a natural approach is to consider all different subsets of these 2
attributes. For convenience, we will refer to the automatic configuration as the Vanilla configuration.
This implies that our possible subsets are 
1. Vanilla, 
2. Vanilla + customer(username), 
3. Vanilla + p order(transaction time), 
4. Vanilla + customer(username) + p order(transaction time). 

Update: we also give an explanation why each of the above can potentially be a good selection: 
1. This could be a good configuration (and indeed the best empirically) because it involves all the attributes used in the join operations. 
It is reasonable to expect that the join operation is one of the more expensive operation among all operations, and therefore it makes
sense to try to optimize this more expensive part first. Additionally, item_id is also heavily used in the where clause, and so the 
Vanilla configuration optimizes the where operation as well.
2. This configuration, from the hindsight, probably shouldn't have a big effect on the performance. This is due to the fact that username 
only appeared in the select clause of the query and so it is not actively searched for during either the join or the where clause. So, 
in principle, it shouldn't make a big difference on the result, we are only including it for sake of experiment and completeness. 
3. This could be a good configuration because it builds an index for transaction_time, which appeared in the where clause. With this additional
index, everything in the where clause of the query is now covered. This means that when executing the where clause, the engine can search for every
attributes through the index, instead of a table scan. Empirically, this indeed turns out to be a minor improvement over the Vanilla configuration.
4. We thought this could be a good configuration because everything that ever appeared in the query is now indexed. In principle, this means the 
engine has the potential to turn every full table scan into a simple index look up, and thus avoiding scanning the table altogether. 

We explored all 4 and summarize our results as follows:

#### 1) (Vanilla):
##### Query 2 with the Vanilla configuration
![Query 2 with the Vanilla configuration](https://github-dev.cs.illinois.edu/sp22-cs411/sp22-cs411-team059-TeamAhYesYes/blob/main/doc/img/indexes_analysis_7.jpg?raw=true)

This would be the baseline model we use, as the indexes are automatically generated by the
indexes optimizer and none of the indexes can be removed.

#### 2) (Vanilla + customer(username)):
##### Query 2 with indexes Vanilla + customer(username)
![Query 2 with indexes Vanilla + customer(username)](https://github-dev.cs.illinois.edu/sp22-cs411/sp22-cs411-team059-TeamAhYesYes/blob/main/doc/img/indexes_analysis_8.jpg?raw=true)

When compared with the baseline, we see that all 4 operations perform better, although all not
by a big margin. In particular, each operation is just better by around 50-300 milliseconds, which
canâ€™t really justify the cost associated with maintaining such an index. Therefore, we consider
this schema viable but not optimal.

#### 3) (Vanilla + p order(transaction time)):
##### Query 2 with indexes Vanilla + p order(transaction time)
![Query 2 with indexes Vanilla + p order(transaction time)](https://github-dev.cs.illinois.edu/sp22-cs411/sp22-cs411-team059-TeamAhYesYes/blob/main/doc/img/indexes_analysis_9.jpg?raw=true)

When compared with the baseline, we see that all 4 operations perform better, although all not
by a big margin. In fact, if we cross-compare this with our previous schema, we see that this
schema and the previous one bring out essentially the same changes. We have already argued
why such savings are likely not worth the cost. Therefore, we consider this schema also as viable
but not optimal.

#### 4) (Vanilla + customer(username) + p order(transaction time)):
##### Query 2 with indexes Vanilla + customer(username) + p order(transaction time)
![Query 2 with indexes Vanilla + customer(username) + p order(transaction time)](https://github-dev.cs.illinois.edu/sp22-cs411/sp22-cs411-team059-TeamAhYesYes/blob/main/doc/img/indexes_analysis_10.jpg?raw=true)

This example really shows that creating indexes isnâ€™t always going to help save time. When
compared with the baseline, we were surprised to find that all 3 operations except the second
one are worse-off, and by huge margins. All 3 operations are (surprisingly) uniformly worse by
a factor of 8 times, and this really says that even without considering the cost associated with
maintaining indexes, creating additional indexes isnâ€™t always going to help. We surmise that this
is likely due to the fact that the Vanilla configuration already includes a lot of automatically
created indexes, and those are already sufficient for the purpose of executing this query.

After considering all 4 schemas, we eventually decided that the Vanilla configuration is the winner.
In other words, we will trust the Indexes optimizer and only use the automatically created indexes.
Although schemas 2 and 3 are comparable and marginally better than the Vanilla configuration, it
isnâ€™t a significant margin and probably is outweighed in the long-run by the cost of maintaining these
additional indexes.
